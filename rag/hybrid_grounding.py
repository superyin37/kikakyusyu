#!/usr/bin/env python3
"""
Hybridå“åæŒ‡ç§°æ¨¡å—
å®ç°åŒè·¯å¾„å“åå€™é€‰ç”Ÿæˆï¼šè·¯å¾„Aï¼ˆæ•´ä½“Embeddingï¼‰+ è·¯å¾„Bï¼ˆLLMè¿‡æ»¤ï¼‰

æœ¬æ¨¡å—ç‹¬ç«‹äºç°æœ‰RAGç³»ç»Ÿï¼Œå¯å•ç‹¬æµ‹è¯•å’ŒéªŒè¯
"""

import json
from typing import List, Dict, Tuple, Optional
import ollama
import chromadb
from dataclasses import dataclass, asdict
import time


# ========== æ•°æ®ç»“æ„å®šä¹‰ ==========

@dataclass
class Candidate:
    """å“åå€™é€‰ç»“æ„"""
    item_name: str          # å“å
    similarity: float       # ç›¸ä¼¼åº¦åˆ†æ•°
    source: str            # æ¥æºè·¯å¾„ ("path_a" | "path_b" | "both")
    metadata: Dict         # åŸå§‹metadataï¼ˆå‡ºã—æ–¹ã€å‚™è€ƒç­‰ï¼‰
    
    def to_dict(self):
        return asdict(self)


@dataclass
class GroundingResult:
    """æŒ‡ç§°ç»“æœ"""
    candidates: List[Candidate]     # å€™é€‰åˆ—è¡¨ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰
    primary_candidate: Optional[Candidate]  # ä¸»å€™é€‰ï¼ˆç½®ä¿¡åº¦æœ€é«˜ï¼‰
    is_ambiguous: bool             # æ˜¯å¦å­˜åœ¨æ­§ä¹‰
    confidence_level: str          # ç½®ä¿¡åº¦çº§åˆ« ("high" | "medium" | "low")
    execution_time_ms: float       # æ‰§è¡Œè€—æ—¶
    path_used: str                # ä½¿ç”¨çš„è·¯å¾„ ("both" | "path_a_only" | "degraded")
    
    def to_dict(self):
        return {
            "candidates": [c.to_dict() for c in self.candidates],
            "primary_candidate": self.primary_candidate.to_dict() if self.primary_candidate else None,
            "is_ambiguous": self.is_ambiguous,
            "confidence_level": self.confidence_level,
            "execution_time_ms": self.execution_time_ms,
            "path_used": self.path_used
        }


# ========== é…ç½®å¸¸é‡ ==========

class HybridConfig:
    """Hybridç³»ç»Ÿé…ç½®"""
    
    # è·¯å¾„Aå‚æ•°
    PATH_A_TOP_K = 3
    
    # è·¯å¾„Bå‚æ•°
    PATH_B_MAX_CANDIDATES = 5
    PATH_B_TOP_K = 3
    PATH_B_TIMEOUT = 5  # ç§’
    
    # å¿«é€Ÿè·¯å¾„é˜ˆå€¼
    SHORT_INPUT_THRESHOLD = 20  # å­—ç¬¦æ•°
    
    # ç½®ä¿¡åº¦é˜ˆå€¼
    CONFIDENCE_THRESHOLD_HIGH = 0.45   # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
    CONFIDENCE_THRESHOLD_LOW = 0.30    # ä½ç½®ä¿¡åº¦é˜ˆå€¼
    AMBIGUITY_THRESHOLD = 0.05         # æ­§ä¹‰åˆ¤å®šé˜ˆå€¼ï¼ˆTop1ä¸Top2å·®å€¼ï¼‰
    
    # LLMé…ç½®
    LLM_MODEL = "swallow:latest"
    LLM_TEMPERATURE = 0.1  # ä½æ¸©åº¦ä»¥æé«˜ç¨³å®šæ€§


# ========== è·¯å¾„Aï¼šæ•´ä½“EmbeddingæŒ‡ç§° ==========

def path_a_global_embedding(
    user_input: str,
    gomi_collection: chromadb.Collection,
    top_k: int = HybridConfig.PATH_A_TOP_K
) -> List[Candidate]:
    """
    è·¯å¾„Aï¼šå¯¹ç”¨æˆ·è¾“å…¥å…¨æ–‡è¿›è¡ŒEmbeddingï¼Œç›´æ¥åŒ¹é…åƒåœ¾å“å
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        gomi_collection: ChromaDBåƒåœ¾åˆ†ç±»collection
        top_k: è¿”å›çš„å€™é€‰æ•°é‡
        
    Returns:
        å€™é€‰åˆ—è¡¨ï¼ˆCandidateå¯¹è±¡ï¼‰
    """
    try:
        results = gomi_collection.query(
            query_texts=[user_input],
            n_results=top_k
        )
        
        candidates = []
        if results and results["metadatas"] and results["distances"]:
            for meta, distance in zip(results["metadatas"][0], results["distances"][0]):
                # ChromaDBè¿”å›çš„æ˜¯è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ (similarity = 1 - distance)
                similarity = 1.0 - distance
                
                candidates.append(Candidate(
                    item_name=meta.get("å“å", ""),
                    similarity=similarity,
                    source="path_a",
                    metadata=meta
                ))
        
        print(f"  è·¯å¾„A: è¿”å›{len(candidates)}ä¸ªå€™é€‰")
        return candidates
        
    except Exception as e:
        print(f"âš ï¸ è·¯å¾„Aæ‰§è¡Œå¤±è´¥: {e}")
        return []


# ========== è·¯å¾„Bï¼šLLMå€™é€‰è¿‡æ»¤ ==========

def path_b_llm_filter(
    user_input: str,
    gomi_collection: chromadb.Collection,
    max_candidates: int = HybridConfig.PATH_B_MAX_CANDIDATES,
    top_k: int = HybridConfig.PATH_B_TOP_K
) -> List[Candidate]:
    """
    è·¯å¾„Bï¼šä½¿ç”¨LLMä»è¾“å…¥ä¸­æå–å¯èƒ½çš„åƒåœ¾å“åçŸ­è¯­ï¼Œç„¶ååˆ†åˆ«EmbeddingåŒ¹é…
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        gomi_collection: ChromaDBåƒåœ¾åˆ†ç±»collection
        max_candidates: LLMæœ€å¤šæå–çš„å€™é€‰çŸ­è¯­æ•°
        top_k: æ¯ä¸ªå€™é€‰çŸ­è¯­åŒ¹é…çš„Top-Kæ•°é‡
        
    Returns:
        å€™é€‰åˆ—è¡¨ï¼ˆCandidateå¯¹è±¡ï¼‰
    """
    try:
        # Step 1: LLMæå–å€™é€‰çŸ­è¯­
        extracted_phrases = _extract_phrases_with_llm(user_input, max_candidates)
        
        if not extracted_phrases:
            print(f"  è·¯å¾„B: LLMæœªæå–åˆ°å€™é€‰çŸ­è¯­")
            return []
        
        print(f"  è·¯å¾„B: LLMæå–åˆ°{len(extracted_phrases)}ä¸ªçŸ­è¯­: {extracted_phrases}")
        
        # Step 2: å¯¹æ¯ä¸ªå€™é€‰çŸ­è¯­è¿›è¡ŒEmbeddingåŒ¹é…
        all_candidates = []
        
        for phrase in extracted_phrases:
            results = gomi_collection.query(
                query_texts=[phrase],
                n_results=top_k
            )
            
            if results and results["metadatas"] and results["distances"]:
                for meta, distance in zip(results["metadatas"][0], results["distances"][0]):
                    similarity = 1.0 - distance
                    
                    all_candidates.append(Candidate(
                        item_name=meta.get("å“å", ""),
                        similarity=similarity,
                        source=f"path_b:{phrase}",  # è®°å½•æ¥æºçŸ­è¯­
                        metadata=meta
                    ))
        
        # Step 3: å»é‡å¹¶æ’åº
        unique_candidates = _deduplicate_candidates(all_candidates)
        unique_candidates.sort(key=lambda c: c.similarity, reverse=True)
        
        result_candidates = unique_candidates[:top_k]
        print(f"  è·¯å¾„B: è¿”å›{len(result_candidates)}ä¸ªå€™é€‰ï¼ˆå»é‡åï¼‰")
        
        return result_candidates
        
    except Exception as e:
        print(f"âš ï¸ è·¯å¾„Bæ‰§è¡Œå¤±è´¥: {e}")
        return []


def _extract_phrases_with_llm(
    user_input: str,
    max_candidates: int
) -> List[str]:
    """
    ä½¿ç”¨LLMä»ç”¨æˆ·è¾“å…¥ä¸­æå–å¯èƒ½çš„åƒåœ¾å“åçŸ­è¯­
    
    Returns:
        å€™é€‰çŸ­è¯­åˆ—è¡¨ï¼ˆæœ€å¤šmax_candidatesä¸ªï¼‰
    """
    prompt = f"""ã‚ãªãŸã¯åŒ—ä¹å·å¸‚ã®ã”ã¿åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰ã€æ¨ã¦ãŸã„ç‰©å“ã®åç§°ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
1. ç‰©å“åã®ã¿ã‚’æŠ½å‡ºï¼ˆèª¬æ˜æ–‡ã‚„å‹•è©ã¯å«ã‚ãªã„ï¼‰
2. æœ€å¤§{max_candidates}å€‹ã¾ã§
3. JSONå½¢å¼ã§å‡ºåŠ›: {{"candidates": ["ç‰©å“1", "ç‰©å“2"]}}
4. å€™è£œãŒãªã„å ´åˆ: {{"candidates": []}}

ã€å…¥åŠ›ã€‘
{user_input}

ã€å‡ºåŠ›ã€‘ï¼ˆJSONå½¢å¼ã®ã¿ã€èª¬æ˜ä¸è¦ï¼‰
"""

    try:
        response = ollama.chat(
            model=HybridConfig.LLM_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯ç‰©å“åæŠ½å‡ºã®å°‚é–€ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            options={
                "temperature": HybridConfig.LLM_TEMPERATURE
            }
        )
        
        content = response["message"]["content"]
        
        # JSONè§£æï¼ˆè¤‡æ•°ã®å½¢å¼ã«å¯¾å¿œï¼‰
        # Case 1: ç›´æ¥JSON
        if content.strip().startswith("{"):
            data = json.loads(content)
            return data.get("candidates", [])[:max_candidates]
        
        # Case 2: Markdown code blockå†…ã®JSON
        if "```json" in content:
            json_str = content.split("```json")[1].split("```")[0].strip()
            data = json.loads(json_str)
            return data.get("candidates", [])[:max_candidates]
        
        # Case 3: å˜ã«```ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        if "```" in content:
            json_str = content.split("```")[1].strip()
            data = json.loads(json_str)
            return data.get("candidates", [])[:max_candidates]
        
        # Case 4: è§£æå¤±æ•—
        print(f"âš ï¸ LLMå‡ºåŠ›ã®è§£æå¤±æ•—: {content[:100]}")
        return []
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸ LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def _deduplicate_candidates(candidates: List[Candidate]) -> List[Candidate]:
    """
    å€™è£œã®é‡è¤‡é™¤å»ï¼ˆåŒã˜å“åã®å ´åˆã€æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’ä¿æŒï¼‰
    """
    seen = {}
    for c in candidates:
        if c.item_name not in seen or c.similarity > seen[c.item_name].similarity:
            seen[c.item_name] = c
    
    return list(seen.values())


# ========== å€™è£œåˆä½µã¨æ±ºç­– ==========

def merge_candidates(
    candidates_a: List[Candidate],
    candidates_b: List[Candidate]
) -> List[Candidate]:
    """
    è·¯å¾„Aå’Œè·¯å¾„Bçš„å€™é€‰è¿›è¡Œåˆå¹¶ã€å»é‡ã€é‡æ’åº
    
    ç­–ç•¥ï¼š
    1. åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
    2. ç›¸åŒå“åçš„å€™é€‰ï¼Œæƒé‡æå‡ï¼ˆå–å¹³å‡åˆ†+0.1ï¼‰
    3. æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
    """
    all_candidates = {}
    
    # å¤„ç†è·¯å¾„Aå€™é€‰
    for c in candidates_a:
        all_candidates[c.item_name] = {
            "candidate": c,
            "sources": ["path_a"],
            "scores": [c.similarity]
        }
    
    # å¤„ç†è·¯å¾„Bå€™é€‰ï¼ˆå¦‚æœå“åå·²å­˜åœ¨ï¼Œæ ‡è®°ä¸ºåŒè·¯å¾„å‘½ä¸­ï¼‰
    for c in candidates_b:
        if c.item_name in all_candidates:
            # åŒè·¯å¾„å‘½ä¸­ - æå‡ç½®ä¿¡åº¦
            all_candidates[c.item_name]["sources"].append("path_b")
            all_candidates[c.item_name]["scores"].append(c.similarity)
        else:
            all_candidates[c.item_name] = {
                "candidate": c,
                "sources": ["path_b"],
                "scores": [c.similarity]
            }
    
    # è®¡ç®—æœ€ç»ˆåˆ†æ•°
    merged = []
    for item_name, data in all_candidates.items():
        candidate = data["candidate"]
        
        # åŒè·¯å¾„å‘½ä¸­ï¼šå–å¹³å‡åˆ†+0.1 bonus
        if len(data["sources"]) > 1:
            final_score = sum(data["scores"]) / len(data["scores"]) + 0.1
            candidate.source = "both"
            print(f"  åˆå¹¶: {item_name} åŒè·¯å¾„å‘½ä¸­ -> æå‡åˆ†æ•°è‡³ {final_score:.3f}")
        else:
            final_score = data["scores"][0]
        
        # æ›´æ–°åˆ†æ•°
        candidate.similarity = min(final_score, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1.0
        merged.append(candidate)
    
    # æŒ‰åˆ†æ•°æ’åº
    merged.sort(key=lambda c: c.similarity, reverse=True)
    
    return merged


# ========== ä¸»å‡½æ•°ï¼šHybridå“åæŒ‡ç§° ==========

def hybrid_grounding(
    user_input: str,
    gomi_collection: chromadb.Collection,
    force_full_path: bool = False
) -> GroundingResult:
    """
    Hybridå“åæŒ‡ç§°ä¸»å‡½æ•°
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥
        gomi_collection: ChromaDBåƒåœ¾åˆ†ç±»collection
        force_full_path: å¼ºåˆ¶æ‰§è¡Œå®Œæ•´åŒè·¯å¾„ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
    Returns:
        GroundingResultå¯¹è±¡
    """
    start_time = time.perf_counter()
    
    # å†³å®šæ˜¯å¦ä½¿ç”¨å¿«é€Ÿè·¯å¾„
    use_fast_path = (
        not force_full_path and 
        len(user_input) < HybridConfig.SHORT_INPUT_THRESHOLD
    )
    
    if use_fast_path:
        # å¿«é€Ÿè·¯å¾„ï¼šä»…è·¯å¾„A
        print(f"ğŸš€ å¿«é€Ÿè·¯å¾„ï¼ˆè¾“å…¥é•¿åº¦: {len(user_input)}å­—ç¬¦ï¼‰")
        candidates_a = path_a_global_embedding(user_input, gomi_collection)
        final_candidates = candidates_a
        path_used = "path_a_only"
    else:
        # å®Œæ•´è·¯å¾„ï¼šåŒè·¯å¾„ä¸²è¡Œæ‰§è¡Œ
        print(f"ğŸ” å®Œæ•´è·¯å¾„ï¼ˆè¾“å…¥é•¿åº¦: {len(user_input)}å­—ç¬¦ï¼‰")
        
        # è·¯å¾„A
        candidates_a = path_a_global_embedding(user_input, gomi_collection)
        
        # è·¯å¾„Bï¼ˆå¸¦é™çº§ï¼‰
        try:
            candidates_b = path_b_llm_filter(user_input, gomi_collection)
        except Exception as e:
            print(f"âš ï¸ è·¯å¾„Bå¤±è´¥ï¼Œé™çº§åˆ°è·¯å¾„A: {e}")
            candidates_b = []
        
        # åˆå¹¶å€™é€‰
        if candidates_b:
            print(f"\nåˆå¹¶å€™é€‰:")
            final_candidates = merge_candidates(candidates_a, candidates_b)
            path_used = "both"
        else:
            final_candidates = candidates_a
            path_used = "degraded"
    
    # ç½®ä¿¡åº¦è¯„ä¼°
    confidence_level, is_ambiguous = _evaluate_confidence(final_candidates)
    
    # æ„å»ºç»“æœ
    end_time = time.perf_counter()
    
    result = GroundingResult(
        candidates=final_candidates,
        primary_candidate=final_candidates[0] if final_candidates else None,
        is_ambiguous=is_ambiguous,
        confidence_level=confidence_level,
        execution_time_ms=(end_time - start_time) * 1000,
        path_used=path_used
    )
    
    return result


def _evaluate_confidence(candidates: List[Candidate]) -> Tuple[str, bool]:
    """
    è¯„ä¼°ç½®ä¿¡åº¦çº§åˆ«
    
    Returns:
        (confidence_level, is_ambiguous)
    """
    if not candidates:
        return "low", False
    
    top_score = candidates[0].similarity
    is_ambiguous = False
    
    # æ£€æŸ¥æ­§ä¹‰ï¼ˆTop1å’ŒTop2å·®å€¼å°ï¼‰
    if len(candidates) >= 2:
        score_diff = top_score - candidates[1].similarity
        is_ambiguous = score_diff < HybridConfig.AMBIGUITY_THRESHOLD
    
    # ç½®ä¿¡åº¦åˆ†çº§
    if top_score >= HybridConfig.CONFIDENCE_THRESHOLD_HIGH:
        confidence_level = "high"
    elif top_score >= HybridConfig.CONFIDENCE_THRESHOLD_LOW:
        confidence_level = "medium"
    else:
        confidence_level = "low"
    
    return confidence_level, is_ambiguous


# ========== å·¥å…·å‡½æ•° ==========

def format_grounding_result(result: GroundingResult) -> str:
    """
    æ ¼å¼åŒ–æŒ‡ç§°ç»“æœç”¨äºæ—¥å¿—è¾“å‡º
    """
    lines = [
        f"\n{'='*60}",
        f"Hybrid Grounding ç»“æœ",
        f"{'='*60}",
        f"æ‰§è¡Œè€—æ—¶: {result.execution_time_ms:.2f}ms",
        f"è·¯å¾„ä½¿ç”¨: {result.path_used}",
        f"ç½®ä¿¡åº¦: {result.confidence_level}",
        f"æ˜¯å¦æ­§ä¹‰: {result.is_ambiguous}",
        f"\nå€™é€‰åˆ—è¡¨ (å…±{len(result.candidates)}ä¸ª):"
    ]
    
    for i, c in enumerate(result.candidates, 1):
        lines.append(
            f"  {i}. {c.item_name} (score: {c.similarity:.3f}, source: {c.source})"
        )
    
    if result.primary_candidate:
        lines.append(f"\nâœ… ä¸»å€™é€‰: {result.primary_candidate.item_name}")
        lines.append(f"   å‡ºã—æ–¹: {result.primary_candidate.metadata.get('å‡ºã—æ–¹', 'N/A')}")
    
    lines.append(f"{'='*60}\n")
    
    return "\n".join(lines)


# ========== å‘½ä»¤è¡Œæ¥å£ ==========

if __name__ == "__main__":
    import sys
    import os
    
    # æ·»åŠ ragç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(__file__))
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python hybrid_grounding.py <ç”¨æˆ·è¾“å…¥>")
        print('ç¤ºä¾‹: python hybrid_grounding.py "ãƒãƒ¼ãƒˆãƒ‘ã‚½ã‚³ãƒ³ã‚’æ¨ã¦ãŸã„"')
        sys.exit(1)
    
    user_input = " ".join(sys.argv[1:])
    
    # åŠ è½½collection
    from rag_demo3 import load_jsonl, build_chroma
    
    print("åˆå§‹åŒ–ChromaDB...")
    gomi_docs, gomi_meta = load_jsonl("rag_docs_merged.jsonl", key="å“å")
    gomi_collection = build_chroma(gomi_docs, gomi_meta, name="gomi")
    
    print(f"\nå¤„ç†è¾“å…¥: {user_input}\n")
    
    # æ‰§è¡ŒHybridæŒ‡ç§°
    result = hybrid_grounding(user_input, gomi_collection, force_full_path=True)
    
    # æ‰“å°ç»“æœ
    print(format_grounding_result(result))
