from fastapi.responses import StreamingResponse
import uvicorn
from fastapi import FastAPI
from typing import Dict
from .schemas import PromptRequest, ReplyResponse
import ollama
import os
import sys
import time
import json
from pathlib import Path

# rag ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ import ã§ãã‚‹ã‚ˆã†ã«ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "rag")))
from rag_demo3 import load_jsonl, build_chroma, rag_retrieve_extended, ask_ollama

app = FastAPI()

import chromadb

# ---- Global paths ----
BASE_DIR = Path(__file__).resolve().parent
RAG_DIR = BASE_DIR.parent / "rag"
CHROMA_PATH = BASE_DIR / "chroma_db"

GOMI_JSONL = RAG_DIR / "rag_docs_merged.jsonl"
AREA_JSONL = RAG_DIR / "area.jsonl"

# ---- 1. Initialize ChromaDB client (only once) ----
chroma_client = chromadb.PersistentClient(path=str(CHROMA_PATH))


# ---- 2. Utility: get existing collection or build if missing ----
def get_or_build_collection(
    client: chromadb.Client,
    name: str,
    docs: list[str] | None = None,
    meta: list[dict] | None = None,
):
    """
    Try to get an existing collection.
    If it does not exist and docs/meta are provided, build the collection.
    Otherwise, raise an error.
    """
    try:
        return client.get_collection(name)
    except Exception:
        if docs is None or meta is None:
            raise RuntimeError(
                f"Collection '{name}' not found and no data provided to build it."
            )
        return build_chroma(docs, meta, name=name)


# =========================
# gomi collection (garbage rules)
# =========================

# Load documents and metadata from JSONL
gomi_docs, gomi_meta = load_jsonl(
    os.path.abspath(GOMI_JSONL),
    key="å“å",
)

# Get existing collection or build it if missing
gomi_collection = get_or_build_collection(
    client=chroma_client,
    name="gomi",
    docs=gomi_docs,
    meta=gomi_meta,
)

# Extract item names for exact / candidate matching
# ========== æ³¨æ„ï¼šHybrid Grounding ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ä¸è¦ ==========
# known_items = [m.get("å“å", "") for m in gomi_meta]
# Hybrid ã‚·ã‚¹ãƒ†ãƒ ãŒç›´æ¥ gomi_collection ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€
# äº‹å‰ã®å“åãƒªã‚¹ãƒˆæ§‹ç¯‰ã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸ


# =========================
# area collection (location / schedule)
# =========================

# Load documents and metadata from JSONL
area_docs, area_meta = load_jsonl(
    os.path.abspath(AREA_JSONL),
    key="ç”ºå",
)

# Get existing collection or build it if missing
area_collection = get_or_build_collection(
    client=chroma_client,
    name="area",
    docs=area_docs,
    meta=area_meta,
)


# =========================
# knowledge collection (user-provided knowledge)
# =========================

# Only try to load the collection; never rebuild automatically
try:
    knowledge_collection = chroma_client.get_collection("knowledge")
except Exception:
    knowledge_collection = None


# =========================
# Debug output (optional)
# =========================

print("gomi collection size:", gomi_collection.count())
print("area collection size:", area_collection.count())
if knowledge_collection:
    print("knowledge collection size:", knowledge_collection.count())
else:
    print("knowledge collection not found")

# # ==== DB æ§‹ç¯‰ ==== (gomi/area ã¯ãã®ã¾ã¾)
# gomi_docs, gomi_meta = load_jsonl(
#     os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "rag", "rag_docs_merged.jsonl")),
#     key="å“å"
# )
# area_docs, area_meta = load_jsonl(
#     os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "rag", "area.jsonl")),
#     key="ç”ºå"
# )

# gomi_collection = build_chroma(gomi_docs, gomi_meta, name="gomi")
# area_collection = build_chroma(area_docs, area_meta, name="area")
# known_items = [m.get("å“å", "") for m in gomi_meta]

# # â† ã“ã“ã‚’è¿½åŠ ï¼
# import chromadb
# client = chromadb.PersistentClient(path="./chroma_db")
# try:
#     knowledge_collection = client.get_collection("knowledge")
# except:
#     knowledge_collection = None



# client = chromadb.PersistentClient(path="./chroma_db")
# col = client.get_or_create_collection("knowledge")

# print(col.count())   # â† ãƒãƒ£ãƒ³ã‚¯æ•°
# print(col.peek(3))   # â† æœ€åˆã®3ä»¶ã‚’è¡¨ç¤º


# ==== ãƒ­ã‚°ä¿å­˜ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
LOG_FILE = Path(os.path.abspath(os.path.join(os.path.dirname(__file__), "logs.jsonl")))

def save_log(user_input: str, assistant_output: str, mode: str):
    log = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "mode": mode,
        "user": user_input,
        "assistant": assistant_output,
    }
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(log, ensure_ascii=False) + "\n")




# ==== Blocking ãƒ¢ãƒ¼ãƒ‰ ====
@app.post("/api/bot/respond")
async def rag_respond(req: PromptRequest):
    # ========== Hybrid Grounding ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ ==========
    # known_items ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸è¦ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ None ã¨ã—ã¦æ¸¡ã™ï¼‰
    rag_prompt, references = rag_retrieve_extended(
        req.prompt,
        gomi_collection,  # â† Hybrid ã‚·ã‚¹ãƒ†ãƒ ãŒç›´æ¥ä½¿ç”¨
        knowledge_collection=knowledge_collection,
        area_collection=area_collection,
        known_items=None,  # â† ä¸è¦ï¼ˆæ—§ç‰ˆã¨ã®äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
        area_meta=area_meta,
        top_k=2
    )
    print("\n===== DEBUG: FULL PROMPT START =====\n")
    print(rag_prompt)
    print("\n===== DEBUG: FULL PROMPT END =====\n")

    reply = ask_ollama(rag_prompt)

    return {
        "reply": reply,
        "references": references  # â† LLMãŒä½¿ã£ãŸ or ãƒãƒƒãƒã—ãŸchunkæƒ…å ±
    }



# ==== Streaming ãƒ¢ãƒ¼ãƒ‰ ====
from fastapi.responses import StreamingResponse
import json

@app.post("/api/bot/respond_stream")
async def rag_respond_stream(req: PromptRequest):
    # ========== Hybrid Grounding ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ + æ€§èƒ½ç›£è¦– ==========
    import time
    retrieval_start = time.perf_counter()
    
    rag_prompt, references = rag_retrieve_extended(
        req.prompt,
        gomi_collection,  # â† Hybrid ã‚·ã‚¹ãƒ†ãƒ ãŒç›´æ¥ä½¿ç”¨
        knowledge_collection=knowledge_collection,
        area_collection=area_collection,
        known_items=None,  # â† ä¸è¦ï¼ˆæ—§ç‰ˆã¨ã®äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
        area_meta=area_meta,
        top_k=2
    )
    
    retrieval_time = (time.perf_counter() - retrieval_start) * 1000
    print(f"\nâ±ï¸  RAGæ¤œç´¢è€—æ™‚: {retrieval_time:.2f}ms")
    print("\n===== DEBUG: FULL PROMPT START =====\n")
    print(rag_prompt)
    print("\n===== DEBUG: FULL PROMPT END =====\n")
    
    # æ€§èƒ½æƒ…å ±ã‚’ references ã«è¿½åŠ 
    if references and isinstance(references, list):
        references.append({
            "type": "performance",
            "retrieval_time_ms": retrieval_time
        })

    def stream_gen():
        collected = ""
        stream = ollama.chat(
            model="swallow:latest",
            messages=[{"role": "user", "content": rag_prompt}],
            stream=True
        )
        for event in stream:
            content = event.get("message", {}).get("content", "")
            if content:
                collected += content
                yield content
        if collected:
            save_log(req.prompt, collected, mode="Streaming(API)")

    # ğŸ“Œ references ã‚’ JSON ã«ã—ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã«åŸ‹ã‚è¾¼ã‚€
    return StreamingResponse(
        stream_gen(),
        media_type="text/plain",
        headers={"X-References": json.dumps(references, ensure_ascii=True)}
    )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
